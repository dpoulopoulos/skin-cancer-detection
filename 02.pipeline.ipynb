{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511178dd-fdb8-46cd-b0b2-2d1ddfcfd43c",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "In this Jupyter Notebook you will create a Kubeflow Pipeline that automates the workflow for the [ISIC 2024 - Skin Cancer Detection with 3D-TBP Kaggle](https://www.kaggle.com/competitions/isic-2024-challenge) competition. The Pipeline consists of two primary steps:\n",
    "\n",
    "1. **📥 Download the Competition Dataset**: In this initial step, the Pipeline downloads the specified Kaggle competition dataset inside a PVC. Utilizing the Kaggle CLI and the previously\n",
    "   created Kubernetes secret, the dataset is retrieved and prepared for subsequent use in the training process.\n",
    "\n",
    "1. **🚀 Launch a Distributed Training Job (PyTorchJob)**: The second step involves launching a distributed training job using the Kubeflow Training operator, specifically, a PyTorchJob.\n",
    "   This step orchestrates the training of a machine learning model in a distributed manner, leveraging the capabilities of PyTorch Distributed for efficient and scalable training.\n",
    "\n",
    "By integrating these steps into a Kubeflow Pipeline, this Notebook facilitates a streamlined, reproducible, and automated approach to training a model for a Kaggle competition. The Pipeline ensures that the dataset is readily available and that the training job is efficiently executed within the Kubeflow environment, providing a robust framework for developing and deploying machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b06325-176e-430e-9b44-7cb945cf72c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional, Union, List\n",
    "\n",
    "from kfp import dsl, compiler, kubernetes, client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fac3cb7-8faf-45b2-841c-d35adfc3dc4e",
   "metadata": {},
   "source": [
    "First, instantiate the Kubeflow Pipelines (KFP) client that you will use to submit the Pipeline.\n",
    "\n",
    "> ❗Ensure that your Notebook has access to the Kubeflow Pipelines component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd377413-c868-4726-ad83-d569f1e53902",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_secret = \"kaggle-secret\"\n",
    "root = Path(\"/\")\n",
    "sa = root/Path(\"var/run/secrets/kubernetes.io/serviceaccount\")\n",
    "ns = open(sa/\"namespace\", \"r\").read()\n",
    "client = client.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181b623-45f6-4f86-bc55-fddf6116cfb2",
   "metadata": {},
   "source": [
    "Each step of the Pipeline is a KFP component. A Pipeline component is self-contained set of code that performs one step in the ML workflow (Pipeline), such as data preprocessing, data transformation, model training, and so on. A component is analogous to a function, in that it has a name, parameters, return values, and a body. Read the [docs](https://www.kubeflow.org/docs/components/pipelines/concepts/component/) to learn more about KFP components.\n",
    "\n",
    "The first component leverages the Kaggle secret you created before to download the dataset to a specified location. You will mount a PVC to this location so you can keep the dataset even after the completion of the Pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccddf83-b628-4c50-958c-2827b1d29680",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=['kaggle==1.6.14'])\n",
    "def download_data(competition: str, data_path: Optional[str] = \"/data\") -> None:\n",
    "    \"\"\"Download the competition dataset from Kaggle to a specified location.\n",
    "\n",
    "    Args:\n",
    "        competition: The name of the Kaggle competition.\n",
    "        data_path: The path where the dataset will be downloaded and extracted. Default is \"/data\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import json\n",
    "    import zipfile\n",
    "    import subprocess\n",
    "    \n",
    "    def init_kaggle() -> None:\n",
    "        # create the Kaggle config directory\n",
    "        kaggle_config_dir = os.path.join(\n",
    "            os.path.expandvars('$HOME'), '.kaggle')\n",
    "        os.makedirs(kaggle_config_dir, exist_ok = True)\n",
    "\n",
    "        # write the `kaggle.json` config file\n",
    "        api_dict = {\n",
    "            \"username\": os.environ['KAGGLE_USERNAME'],\n",
    "            \"key\":os.environ['KAGGLE_KEY']}\n",
    "        with open(os.path.join(kaggle_config_dir, \"kaggle.json\"), \"w\", encoding='utf-8') as f:\n",
    "            json.dump(api_dict, f)\n",
    "\n",
    "        # change `kaggle.json` permissions\n",
    "        cmd = f\"chmod 600 {kaggle_config_dir}/kaggle.json\"\n",
    "        output = subprocess.check_output(cmd.split(\" \"))\n",
    "        \n",
    "    init_kaggle()\n",
    "    \n",
    "    import kaggle\n",
    "    \n",
    "    # download the competition files\n",
    "    kaggle.api.competition_download_files(competition, path=data_path)\n",
    "    with zipfile.ZipFile(os.path.join(data_path, f\"{competition}.zip\"), 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce895d7-7c71-413c-896d-3a9b70ae7601",
   "metadata": {},
   "source": [
    "The second component launches a distributed training job using the Kubeflow Training Operator Python client. This component generates the Custom Resource (CR) using the corresponding SDK and submits it to Kubernetes. The resulting PyTorchJob comprises `Master` and `Worker` replicas (one of each), which share the training load, thereby speeding up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f9c4bc-b575-4dd9-b6a5-63c5b87776e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=[\"kubeflow-training==1.8.0\"])\n",
    "def launch_training(\n",
    "    run_name: str,\n",
    "    namespace: str,\n",
    "    data_vol: str,\n",
    "    logs_vol: str,\n",
    "    image: str,\n",
    "    image_cmd: Optional[List[str]] = list(),\n",
    "    image_args: Optional[List[str]] = list(),\n",
    "    data_mount_path: Optional[str] = \"/data\",\n",
    "    logs_mount_path: Optional[str] = \"/logs\",\n",
    ") -> None:\n",
    "    \"\"\"Launch a distributed training job using the Kubeflow Training Operator.\n",
    "\n",
    "    Args:\n",
    "        run_name: The name of the training run.\n",
    "        namespace: The Kubernetes namespace where the job will be created.\n",
    "        data_vol: The name of the Persistent Volume Claim (PVC) for the data.\n",
    "        logs_vol: The name of the Persistent Volume Claim (PVC) for the logs.\n",
    "        image: The Docker image to use for the training job.\n",
    "        image_cmd: The command to run in the Docker image. Default is an empty list.\n",
    "        image_args: The arguments to pass to the command. Default is an empty list.\n",
    "        data_mount_path: The path to mount the data volume inside the container. Default is \"/data\".\n",
    "        logs_mount_path: The path to mount the logs volume inside the container. Default is \"/logs\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    from kubeflow.training import TrainingClient, constants\n",
    "    from kubernetes.client import (V1ObjectMeta,\n",
    "                                   V1PodTemplateSpec,\n",
    "                                   V1PodSpec,\n",
    "                                   V1Volume,\n",
    "                                   V1PersistentVolumeClaimVolumeSource,\n",
    "                                   V1EmptyDirVolumeSource,\n",
    "                                   V1Container,\n",
    "                                   V1VolumeMount,\n",
    "                                   V1ResourceRequirements)\n",
    "    from kubeflow.training.models import (KubeflowOrgV1PyTorchJob,\n",
    "                                          KubeflowOrgV1PyTorchJobSpec,\n",
    "                                          KubeflowOrgV1ReplicaSpec,\n",
    "                                          KubeflowOrgV1RunPolicy)\n",
    "    \n",
    "    training_client = TrainingClient(job_kind=constants.PYTORCHJOB_KIND)\n",
    "    \n",
    "    def _get_metadata(name: str = None, annotations: dict = None) -> V1ObjectMeta:\n",
    "        return V1ObjectMeta(name=name, annotations=annotations)\n",
    "    \n",
    "    def _get_volume(\n",
    "        name: str, \n",
    "        persistent_volume_claim: V1PersistentVolumeClaimVolumeSource = None,\n",
    "        empty_dir: V1EmptyDirVolumeSource = None\n",
    "    ) -> V1Volume:\n",
    "        return V1Volume(name=name, persistent_volume_claim=persistent_volume_claim, empty_dir=empty_dir)\n",
    "    \n",
    "    def _get_volume_mount(name: str, mount_path: str) -> V1VolumeMount:\n",
    "        return V1VolumeMount(name=name, mount_path=mount_path)\n",
    "    \n",
    "    # define job's metadata\n",
    "    pytorch_job_metadata = _get_metadata(name=run_name)\n",
    "    pytorch_replica_metadata = _get_metadata(\n",
    "        annotations={\"sidecar.istio.io/inject\": \"false\"})\n",
    "    \n",
    "    # define volumes\n",
    "    data_volume = _get_volume(\n",
    "        data_vol, persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(claim_name=data_vol))\n",
    "    logs_volume = _get_volume(\n",
    "        logs_vol, persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(claim_name=logs_vol))\n",
    "    shm_volume = _get_volume(\n",
    "        \"dshm\", empty_dir=V1EmptyDirVolumeSource(medium=\"Memory\", size_limit=\"2Gi\"))\n",
    "    \n",
    "    # define volume mounts\n",
    "    data_volume_mount = _get_volume_mount(data_vol, data_mount_path)\n",
    "    logs_volume_mount = _get_volume_mount(logs_vol, logs_mount_path)\n",
    "    dshm_volume_mount = _get_volume_mount(\"dshm\", \"/dev/shm\")\n",
    "    \n",
    "    # define job's container\n",
    "    pytorch_replica_container = V1Container(\n",
    "        name=\"pytorch\",\n",
    "        image=image,\n",
    "        command=image_cmd,\n",
    "        args=image_args,\n",
    "        resources=V1ResourceRequirements(\n",
    "            limits={\"nvidia.com/gpu\": \"1\"}),\n",
    "        volume_mounts=[data_volume_mount, logs_volume_mount, dshm_volume_mount])\n",
    "    \n",
    "    # define job's replica spec\n",
    "    pytorch_replica_template_spec = V1PodSpec(\n",
    "        volumes=[data_volume, logs_volume, shm_volume],\n",
    "        containers=[pytorch_replica_container])\n",
    "    pytorch_replica_template = V1PodTemplateSpec(\n",
    "        metadata=pytorch_replica_metadata,\n",
    "        spec=pytorch_replica_template_spec)\n",
    "    pytorch_replica_spec = KubeflowOrgV1ReplicaSpec(\n",
    "        replicas=1,\n",
    "        restart_policy=\"OnFailure\",\n",
    "        template=pytorch_replica_template)\n",
    "    pytorch_replica_specs = {\n",
    "        \"Master\": pytorch_replica_spec,\n",
    "        \"Worker\": pytorch_replica_spec\n",
    "    }\n",
    "    \n",
    "    # define PyTorchJob spec\n",
    "    pytorch_job_spec = KubeflowOrgV1PyTorchJobSpec(\n",
    "        pytorch_replica_specs=pytorch_replica_specs,\n",
    "        run_policy=KubeflowOrgV1RunPolicy())\n",
    "    \n",
    "    pytorch_job = KubeflowOrgV1PyTorchJob(\n",
    "        api_version=\"kubeflow.org/v1\",\n",
    "        kind=\"PyTorchJob\",\n",
    "        metadata=pytorch_job_metadata,\n",
    "        spec=pytorch_job_spec)\n",
    "    \n",
    "    training_client.create_job(pytorch_job, namespace=namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff6bfa6-5b9d-4f45-a19e-f3b0e1e7d87c",
   "metadata": {},
   "source": [
    "Finally, you are ready to create the Pipeline. This Pipeline includes two additional steps: one to create a PVC for the dataset download in the first step, and another to create a PVC for logging the training progress of the distributed training job. You can use the second PVC to launch a TensorBoard instance to monitor the training run.\n",
    "\n",
    "Moreover, the Pipeline takes care of a few other details, such as passing the necessary environment variables to each Pod, defining the order in which each step should run, and determining whether to cache a step or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb45234-ad9d-451c-b053-156108074210",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline\n",
    "def isic_pipeline(\n",
    "    namespace: str,\n",
    "    competition_name: str,\n",
    "    dist_run_name: str,\n",
    "    data_vol: str,\n",
    "    logs_vol: str,\n",
    "    dist_run_image: str,\n",
    "    data_path: Optional[str] = \"/data\",\n",
    "    dist_image_cmd: Optional[List[str]] = list(),\n",
    "    dist_image_args: Optional[List[str]] = list(),\n",
    "    data_mount_path: Optional[str] = \"/data\",\n",
    "    logs_mount_path: Optional[str] = \"/logs\",\n",
    ") -> None:\n",
    "    \"\"\"Define a KFP Pipeline for downloading competition data and launching a distributed training job.\n",
    "\n",
    "    Args:\n",
    "        namespace: The Kubernetes namespace where the Pipeline will run.\n",
    "        competition_name: The name of the Kaggle competition to download data from.\n",
    "        dist_run_name: The name of the distributed training run.\n",
    "        data_vol: The name of the Persistent Volume Claim (PVC) for the data.\n",
    "        logs_vol: The name of the Persistent Volume Claim (PVC) for the logs.\n",
    "        dist_run_image: The Docker image to use for the distributed training job.\n",
    "        data_path: The path where the dataset will be downloaded and extracted. Default is \"/data\".\n",
    "        dist_image_cmd: The command to run in the Docker image for the training job. Default is an empty list.\n",
    "        dist_image_args: The arguments to pass to the command for the training job. Default is an empty list.\n",
    "        data_mount_path: The path to mount the data volume inside the container. Default is \"/data\".\n",
    "        logs_mount_path: The path to mount the logs volume inside the container. Default is \"/logs\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # create a PVC to store the dataset\n",
    "    isic_data_pvc = kubernetes.CreatePVC(\n",
    "        pvc_name='isic-data',\n",
    "        access_modes=['ReadWriteMany'],\n",
    "        size='8.0Gi',\n",
    "        storage_class_name='longhorn'\n",
    "    )\n",
    "    \n",
    "    # create a PVC to log the training progress\n",
    "    isic_logs_pvc = kubernetes.CreatePVC(\n",
    "        pvc_name='isic-logs',\n",
    "        access_modes=['ReadWriteMany'],\n",
    "        size='2.0Gi',\n",
    "        storage_class_name='longhorn'\n",
    "    )\n",
    "\n",
    "    download_data_step = download_data(\n",
    "        competition=competition_name,\n",
    "        data_path=data_path).after(isic_data_pvc)\n",
    "    download_data_step.set_caching_options(enable_caching=True)\n",
    "    \n",
    "    launch_training_step = launch_training(\n",
    "        run_name=dist_run_name,\n",
    "        namespace=namespace,\n",
    "        data_vol=data_vol,\n",
    "        logs_vol=logs_vol,\n",
    "        image=dist_run_image,\n",
    "        image_cmd=dist_image_cmd,\n",
    "        image_args=dist_image_args,\n",
    "        data_mount_path=data_mount_path,\n",
    "        logs_mount_path=logs_mount_path).after(download_data_step)\n",
    "    launch_training_step.set_caching_options(enable_caching=False)\n",
    "\n",
    "    kubernetes.mount_pvc(\n",
    "        download_data_step,\n",
    "        pvc_name=isic_data_pvc.outputs['name'],\n",
    "        mount_path='/data')\n",
    "    kubernetes.use_secret_as_env(\n",
    "        download_data_step,\n",
    "        secret_name=kaggle_secret,\n",
    "        secret_key_to_env={'username': 'KAGGLE_USERNAME'})\n",
    "    kubernetes.use_secret_as_env(\n",
    "        download_data_step,\n",
    "        secret_name=kaggle_secret,\n",
    "        secret_key_to_env={'key': 'KAGGLE_KEY'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e95ed-d7b4-4fd8-84e4-a2eb4690ff1e",
   "metadata": {},
   "source": [
    "You are almost ready to launch your experiment. First, compile the Pipeline into an intermediate representation YAML file. Then, create an Experiment to group several runs. Finally, create a Run from your Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dda429-0b68-4f8a-83f3-c8e13dc8c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(isic_pipeline, package_path='pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f5bf5d-ae59-4ca0-b047-07f0be725a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = client.create_experiment(\n",
    "    name=\"isic-experiment\",\n",
    "    description=\"Skin Cancer Detection with 3D-TBP\",\n",
    "    namespace=ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d09b94-fb06-438c-acd3-50f1a04004f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = client.create_run_from_pipeline_package(\n",
    "    pipeline_file=\"pipeline.yaml\",\n",
    "    experiment_name=experiment.display_name,\n",
    "    namespace=ns,\n",
    "    run_name=\"isic-run\",\n",
    "    arguments={\n",
    "        \"namespace\": ns,\n",
    "        \"competition_name\": \"isic-2024-challenge\",\n",
    "        \"dist_run_name\": \"pytorch-dist-isic-efficientnet\",\n",
    "        \"data_vol\": \"isic-data\",\n",
    "        \"logs_vol\": \"isic-logs\",\n",
    "        \"dist_run_image\": \"dpoulopoulos/pytorch-dist-isic:61a89cd\",\n",
    "    },\n",
    ")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
