{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b06325-176e-430e-9b44-7cb945cf72c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, List\n",
    "\n",
    "from pathlib import Path\n",
    "import kfp.typing\n",
    "from kfp import dsl, compiler, kubernetes, client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd377413-c868-4726-ad83-d569f1e53902",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_secret = \"kaggle-secret\"\n",
    "root = Path(\"/\")\n",
    "sa = root/Path(\"var/run/secrets/kubernetes.io/serviceaccount\")\n",
    "ns = open(sa/\"namespace\", \"r\").read()\n",
    "client = client.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccddf83-b628-4c50-958c-2827b1d29680",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=['kaggle==1.6.14'])\n",
    "def download_data(competition: str, data_path: Optional[str] = \"/data\") -> None:\n",
    "    import os\n",
    "    import json\n",
    "    import zipfile\n",
    "    import subprocess\n",
    "    \n",
    "    def init_kaggle() -> None:\n",
    "        # create the Kaggle config directory\n",
    "        kaggle_config_dir = os.path.join(\n",
    "            os.path.expandvars('$HOME'), '.kaggle')\n",
    "        os.makedirs(kaggle_config_dir, exist_ok = True)\n",
    "\n",
    "        # write the `kaggle.json` config file\n",
    "        api_dict = {\n",
    "            \"username\": os.environ['KAGGLE_USERNAME'],\n",
    "            \"key\":os.environ['KAGGLE_KEY']}\n",
    "        with open(os.path.join(kaggle_config_dir, \"kaggle.json\"), \"w\", encoding='utf-8') as f:\n",
    "            json.dump(api_dict, f)\n",
    "\n",
    "        # change `kaggle.json` permissions\n",
    "        cmd = f\"chmod 600 {kaggle_config_dir}/kaggle.json\"\n",
    "        output = subprocess.check_output(cmd.split(\" \"))\n",
    "        \n",
    "    init_kaggle()\n",
    "    \n",
    "    import kaggle\n",
    "    \n",
    "    # download the competition files\n",
    "    kaggle.api.competition_download_files(competition, path=data_path)\n",
    "    with zipfile.ZipFile(os.path.join(data_path, f\"{competition}.zip\"), 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f9c4bc-b575-4dd9-b6a5-63c5b87776e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=[\"kubeflow-training==1.8.0\"])\n",
    "def launch_training(\n",
    "    run_name: str,\n",
    "    namespace: str,\n",
    "    data_vol: str,\n",
    "    logs_vol: str,\n",
    "    image: str,\n",
    "    image_cmd: Optional[List[str]] = list(),\n",
    "    image_args: Optional[List[str]] = list(),\n",
    "    data_mount_path: Optional[str] = \"/data\",\n",
    "    logs_mount_path: Optional[str] = \"/logs\",\n",
    ") -> None:\n",
    "    from kubeflow.training import TrainingClient, constants\n",
    "    from kubernetes.client import (V1ObjectMeta,\n",
    "                                   V1PodTemplateSpec,\n",
    "                                   V1PodSpec,\n",
    "                                   V1Volume,\n",
    "                                   V1PersistentVolumeClaimVolumeSource,\n",
    "                                   V1EmptyDirVolumeSource,\n",
    "                                   V1Container,\n",
    "                                   V1VolumeMount,\n",
    "                                   V1ResourceRequirements)\n",
    "    from kubeflow.training.models import (KubeflowOrgV1PyTorchJob,\n",
    "                                          KubeflowOrgV1PyTorchJobSpec,\n",
    "                                          KubeflowOrgV1ReplicaSpec,\n",
    "                                          KubeflowOrgV1RunPolicy)\n",
    "    \n",
    "    training_client = TrainingClient(job_kind=constants.PYTORCHJOB_KIND)\n",
    "    \n",
    "    # define job's metadata\n",
    "    pytorch_job_metadata = V1ObjectMeta(name=run_name)\n",
    "    pytorch_replica_metadata = V1ObjectMeta(\n",
    "        annotations={\"sidecar.istio.io/inject\": \"false\"})\n",
    "    \n",
    "    # define volumes\n",
    "    data_volume = V1Volume(\n",
    "        name=data_vol,\n",
    "        persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(claim_name=data_vol))\n",
    "    logs_volume = V1Volume(\n",
    "        name=logs_vol,\n",
    "        persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(claim_name=logs_vol))\n",
    "    shm_volume = V1Volume(\n",
    "        name=\"dshm\",\n",
    "        empty_dir=V1EmptyDirVolumeSource(medium=\"Memory\", size_limit=\"2Gi\"))\n",
    "    \n",
    "    # define volume mounts\n",
    "    data_volume_mount = V1VolumeMount(\n",
    "        name=data_vol,\n",
    "        mount_path=data_mount_path)\n",
    "    logs_volume_mount = V1VolumeMount(\n",
    "        name=logs_vol,\n",
    "        mount_path=logs_mount_path)\n",
    "    dshm_volume_mount = V1VolumeMount(\n",
    "        name=\"dshm\",\n",
    "        mount_path=\"/dev/shm\")\n",
    "    \n",
    "    # define job's container\n",
    "    pytorch_replica_container = V1Container(\n",
    "        name=\"pytorch\",\n",
    "        image=image,\n",
    "        command=image_cmd,\n",
    "        args=image_args,\n",
    "        resources=V1ResourceRequirements(\n",
    "            limits={\"nvidia.com/gpu\": \"1\"}),\n",
    "        volume_mounts=[data_volume_mount, logs_volume_mount, dshm_volume_mount])\n",
    "    \n",
    "    # define job's replica spec\n",
    "    pytorch_replica_template_spec = V1PodSpec(\n",
    "        volumes=[data_volume, logs_volume, shm_volume],\n",
    "        containers=[pytorch_replica_container])\n",
    "    pytorch_replica_template = V1PodTemplateSpec(\n",
    "        metadata=pytorch_replica_metadata,\n",
    "        spec=pytorch_replica_template_spec)\n",
    "    pytorch_replica_spec = KubeflowOrgV1ReplicaSpec(\n",
    "        replicas=1,\n",
    "        restart_policy=\"OnFailure\",\n",
    "        template=pytorch_replica_template)\n",
    "    pytorch_replica_specs = {\n",
    "        \"Master\": pytorch_replica_spec,\n",
    "        \"Worker\": pytorch_replica_spec\n",
    "    }\n",
    "    \n",
    "    # define PyTorchJob spec\n",
    "    pytorch_job_spec = KubeflowOrgV1PyTorchJobSpec(\n",
    "        pytorch_replica_specs=pytorch_replica_specs,\n",
    "        run_policy=KubeflowOrgV1RunPolicy())\n",
    "    \n",
    "    pytorch_job = KubeflowOrgV1PyTorchJob(\n",
    "        api_version=\"kubeflow.org/v1\",\n",
    "        kind=\"PyTorchJob\",\n",
    "        metadata=pytorch_job_metadata,\n",
    "        spec=pytorch_job_spec)\n",
    "    \n",
    "    training_client.create_job(pytorch_job, namespace=namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb45234-ad9d-451c-b053-156108074210",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline\n",
    "def isic_pipeline(\n",
    "    namespace: str,\n",
    "    competition_name: str,\n",
    "    dist_run_name: str,\n",
    "    data_vol: str,\n",
    "    logs_vol: str,\n",
    "    dist_run_image: str,\n",
    "    data_path: Optional[str] = \"/data\",\n",
    "    dist_image_cmd: Optional[List[str]] = list(),\n",
    "    dist_image_args: Optional[List[str]] = list(),\n",
    "    data_mount_path: Optional[str] = \"/data\",\n",
    "    logs_mount_path: Optional[str] = \"/logs\",\n",
    ") -> None:\n",
    "    # create a PVC to store the dataset\n",
    "    isic_data_pvc = kubernetes.CreatePVC(\n",
    "        pvc_name='isic-data',\n",
    "        access_modes=['ReadWriteMany'],\n",
    "        size='8.0Gi',\n",
    "        storage_class_name='longhorn'\n",
    "    )\n",
    "    \n",
    "    # create a PVC to log the training progress\n",
    "    isic_logs_pvc = kubernetes.CreatePVC(\n",
    "        pvc_name='isic-logs',\n",
    "        access_modes=['ReadWriteMany'],\n",
    "        size='2.0Gi',\n",
    "        storage_class_name='longhorn'\n",
    "    )\n",
    "\n",
    "    download_data_step = download_data(\n",
    "        competition=competition_name,\n",
    "        data_path=data_path).after(isic_data_pvc)\n",
    "    download_data_step.set_caching_options(enable_caching=True)\n",
    "    \n",
    "    launch_training_step = launch_training(\n",
    "        run_name=dist_run_name,\n",
    "        namespace=namespace,\n",
    "        data_vol=data_vol,\n",
    "        logs_vol=logs_vol,\n",
    "        image=dist_run_image,\n",
    "        image_cmd=dist_image_cmd,\n",
    "        image_args=dist_image_args,\n",
    "        data_mount_path=data_mount_path,\n",
    "        logs_mount_path=logs_mount_path).after(download_data_step)\n",
    "    launch_training_step.set_caching_options(enable_caching=False)\n",
    "\n",
    "    kubernetes.mount_pvc(\n",
    "        download_data_step,\n",
    "        pvc_name=isic_data_pvc.outputs['name'],\n",
    "        mount_path='/data')\n",
    "    kubernetes.use_secret_as_env(\n",
    "        download_data_step,\n",
    "        secret_name=kaggle_secret,\n",
    "        secret_key_to_env={'username': 'KAGGLE_USERNAME'})\n",
    "    kubernetes.use_secret_as_env(\n",
    "        download_data_step,\n",
    "        secret_name=kaggle_secret,\n",
    "        secret_key_to_env={'key': 'KAGGLE_KEY'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dda429-0b68-4f8a-83f3-c8e13dc8c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(isic_pipeline, package_path='pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f5bf5d-ae59-4ca0-b047-07f0be725a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = client.create_experiment(\n",
    "    name=\"isic-experiment\",\n",
    "    description=\"Skin Cancer Detection with 3D-TBP\",\n",
    "    namespace=ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d09b94-fb06-438c-acd3-50f1a04004f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = client.create_run_from_pipeline_package(\n",
    "    pipeline_file=\"pipeline.yaml\",\n",
    "    experiment_name=experiment.display_name,\n",
    "    namespace=ns,\n",
    "    run_name=\"isic-run\",\n",
    "    arguments={\n",
    "        \"namespace\": ns,\n",
    "        \"competition_name\": \"isic-2024-challenge\",\n",
    "        \"dist_run_name\": \"pytorch-dist-isic-efficientnet\",\n",
    "        \"data_vol\": \"isic-data\",\n",
    "        \"logs_vol\": \"isic-logs\",\n",
    "        \"dist_run_image\": \"dpoulopoulos/pytorch-dist-isic:61a89cd\",\n",
    "    },\n",
    ")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
